{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"report.pdf\"\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are a helpful medical assistant that that can answer questions about a patient based on their report: {docs}\n",
    "\n",
    "Only use the factual information from the report to answer the question.\n",
    "\n",
    "If you feel like you don't have enough information to answer the question, say \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "HUMAN_TEMPLATE = \"\"\"\n",
    "Answer the following question: {question}\n",
    "\"\"\"\n",
    "\n",
    "MODEL = ChatOpenAI(\n",
    "    model_name = \"gpt-3.5-turbo\", \n",
    "    temperature = 0.2,\n",
    "    openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "EMBEDDINGS = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(filename=\"proj4.pdf\"):\n",
    "    reader = PdfReader(filename)\n",
    "    num_pages = len(reader.pages)\n",
    "\n",
    "    temp = []\n",
    "    for i in range(num_pages):\n",
    "        temp.append(reader.pages[i].extract_text())\n",
    "    context = ''.join(temp)\n",
    "    return context\n",
    "\n",
    "\n",
    "def get_embeddings_db(filename=\"proj4.pdf\", chunk_size=2000, chunk_overlap=200):\n",
    "    loader = PyPDFLoader(filename)\n",
    "    pages = loader.load()\n",
    "\n",
    "    # define splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        is_separator_regex = False\n",
    "    )\n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    db = FAISS.from_documents(docs, EMBEDDINGS)\n",
    "    return db\n",
    "\n",
    "def get_system_message_prompt(system_template):\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    return system_message_prompt\n",
    "\n",
    "def get_human_message_prompt(human_template):\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    return human_message_prompt\n",
    "\n",
    "def get_chat_prompt(system_message_prompt, human_message_prompt):\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [system_message_prompt, human_message_prompt]\n",
    "    )\n",
    "    return chat_prompt\n",
    "\n",
    "def generate_response(model, query, db, k=4):\n",
    "    # set up prompts\n",
    "    system_message_prompt  = get_system_message_prompt(SYSTEM_TEMPLATE)\n",
    "    human_message_prompt = get_human_message_prompt(HUMAN_TEMPLATE)\n",
    "    chat_prompt = get_chat_prompt(system_message_prompt, human_message_prompt)\n",
    "\n",
    "    #set up doc search\n",
    "    docs = db.similarity_search(\n",
    "        query, \n",
    "        k = k\n",
    "    )\n",
    "    docs_page_content = \" \".join([d.page_content for d in docs])\n",
    "    \n",
    "    # set up output parser\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    # create chain\n",
    "    chain = LLMChain(\n",
    "        llm = model,\n",
    "        prompt = chat_prompt\n",
    "    )\n",
    "    chain = chat_prompt| model | parser\n",
    "\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"question\": query,\n",
    "            \"docs\": docs_page_content\n",
    "        }\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, the patient, Mr. Tan Ah Kow, has a past medical history of dementia and stroke.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB = get_embeddings_db(\n",
    "    filename=FILENAME, \n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "user_query = \"What is the patient's past medical history?\"\n",
    "response = generate_response(\n",
    "    model = MODEL,\n",
    "    query = user_query,\n",
    "    db = DB,\n",
    "    k = 4\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient, Mr. Tan Ah Kow, has a history of hypertension and hyperlipidemia since 1990. He has also suffered several strokes in 2005, leading to subsequent issues such as cardiomyopathy, cardiac failure, and chronic renal disease. Additionally, he was diagnosed with dementia and has experienced a gradual deterioration in his cognitive ability and physical state over the years.\n"
     ]
    }
   ],
   "source": [
    "DB = get_embeddings_db(\n",
    "    filename=FILENAME, \n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "user_query = \"What is the patient's past medical history?\"\n",
    "response = generate_response(\n",
    "    model = MODEL,\n",
    "    query = user_query,\n",
    "    db = DB,\n",
    "    k = 4\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
